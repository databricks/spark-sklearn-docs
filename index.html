
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to spark_sklearn’s documentation! &#8212; spark_sklearn 0.2.3 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.2.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">spark_sklearn 0.2.3 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="welcome-to-spark-sklearn-s-documentation">
<h1>Welcome to spark_sklearn’s documentation!<a class="headerlink" href="#welcome-to-spark-sklearn-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Contents:</p>
<div class="toctree-wrapper compound">
</div>
<span class="target" id="module-spark_sklearn"></span><dl class="class">
<dt id="spark_sklearn.Converter">
<em class="property">class </em><code class="descclassname">spark_sklearn.</code><code class="descname">Converter</code><span class="sig-paren">(</span><em>sc</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.Converter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Class for converting between scikit-learn and Spark ML models</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sc</strong> – SparkContext</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="spark_sklearn.Converter.toPandas">
<code class="descname">toPandas</code><span class="sig-paren">(</span><em>df</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.Converter.toPandas" title="Permalink to this definition">¶</a></dt>
<dd><p>This is similar to the Spark DataFrame built-in toPandas() method, but it handles
MLlib Vector columns differently.  It converts MLlib Vectors into rows of
scipy.sparse.csr_matrix, which is generally friendlier for PyData tools like scikit-learn.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental: This will likely be replaced in later releases with improved APIs.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> – Spark DataFrame</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Pandas dataframe</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="spark_sklearn.Converter.toSKLearn">
<code class="descname">toSKLearn</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.Converter.toSKLearn" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a Spark MLlib model from the Pipelines API (spark.ml) to a scikit-learn model.
Currently supported models:
- pyspark.ml.classification.LogisticRegressionModel
- pyspark.ml.regression.LinearRegressionModel</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>model</strong> – Spark ML model</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">scikit-learn model with equivalent predictive behavior.
Currently, parameters or arguments for training are not copied.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="spark_sklearn.Converter.toSpark">
<code class="descname">toSpark</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.Converter.toSpark" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a scikit-learn model to a Spark ML model from the Pipelines API (spark.ml).
Currently supported models:
- sklearn.linear_model.LogisticRegression (binary classification only, not multiclass)
- sklearn.linear_model.LinearRegression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>model</strong> – scikit-learn model</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Spark ML model with equivalent predictive behavior.
Currently, parameters or arguments for training are not copied.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="spark_sklearn.CSRVectorUDT">
<em class="property">class </em><code class="descclassname">spark_sklearn.</code><code class="descname">CSRVectorUDT</code><a class="headerlink" href="#spark_sklearn.CSRVectorUDT" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">pyspark.sql.types.UserDefinedType</span></code></p>
<p>SQL user-defined type (UDT) for scipy.sparse.csr_matrix (vectors only, not matrices).</p>
<blockquote>
<div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
</div></blockquote>
</dd></dl>

<dl class="class">
<dt id="spark_sklearn.GridSearchCV">
<em class="property">class </em><code class="descclassname">spark_sklearn.</code><code class="descname">GridSearchCV</code><span class="sig-paren">(</span><em>sc</em>, <em>estimator</em>, <em>param_grid</em>, <em>scoring=None</em>, <em>fit_params=None</em>, <em>n_jobs=1</em>, <em>iid=True</em>, <em>refit=True</em>, <em>cv=None</em>, <em>verbose=0</em>, <em>pre_dispatch='2*n_jobs'</em>, <em>error_score='raise'</em>, <em>return_train_score=True</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.GridSearchCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">sklearn.model_selection._search.BaseSearchCV</span></code></p>
<p>Exhaustive search over specified parameter values for an estimator.</p>
<p>Important members are fit, predict.</p>
<p>GridSearchCV implements a “fit” and a “score” method.
It also implements “predict”, “predict_proba”, “decision_function”,
“transform” and “inverse_transform” if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated grid-search over a parameter grid.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">estimator object.</span></dt>
<dd>This is assumed to implement the scikit-learn estimator interface.
Either estimator needs to provide a <code class="docutils literal"><span class="pre">score</span></code> function,
or <code class="docutils literal"><span class="pre">scoring</span></code> must be passed.</dd>
<dt>param_grid <span class="classifier-delimiter">:</span> <span class="classifier">dict or list of dictionaries</span></dt>
<dd>Dictionary with parameters names (string) as keys and lists of
parameter settings to try as values, or a list of such
dictionaries, in which case the grids spanned by each dictionary
in the list are explored. This enables searching over any sequence
of parameter settings.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, default=None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
If <code class="docutils literal"><span class="pre">None</span></code>, the <code class="docutils literal"><span class="pre">score</span></code> method of the estimator is used.</dd>
<dt>fit_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Parameters to pass to the fit method.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int, default=1</span></dt>
<dd>Number of jobs to run in parallel.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in ‘2*n_jobs’</li>
</ul>
</div></blockquote>
</dd>
<dt>iid <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>If True, the data is assumed to be identically distributed across
the folds, and the loss minimized is the total loss per sample,
and not the mean loss across the folds.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span></dt>
<dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<blockquote>
<div><ul class="simple">
<li>None, to use the default 3-fold cross validation,</li>
<li>integer, to specify the number of folds in a <cite>(Stratified)KFold</cite>,</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train, test splits.</li>
</ul>
</div></blockquote>
<p class="last">For integer/None inputs, if the estimator is a classifier and <code class="docutils literal"><span class="pre">y</span></code> is
either binary or multiclass, <code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> is used. In all
other cases, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.
Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>Refit the best estimator with the entire dataset.
If “False”, it is impossible to make predictions using
this GridSearchCV instance after fitting.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Controls the verbosity: the higher, the more messages.</dd>
<dt>error_score <span class="classifier-delimiter">:</span> <span class="classifier">‘raise’ (default) or numeric</span></dt>
<dd>Value to assign to the score if an error occurs in estimator fitting.
If set to ‘raise’, the error is raised. If a numeric value is given,
FitFailedWarning is raised. This parameter does not affect the refit
step, which will always raise the error.</dd>
<dt>return_train_score <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">'False'</span></code>, the <code class="docutils literal"><span class="pre">cv_results_</span></code> attribute will not include training
scores.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.grid_search</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.util</span> <span class="k">import</span> <span class="n">createLocalSparkSession</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span> <span class="o">=</span> <span class="n">createLocalSparkSession</span><span class="p">()</span><span class="o">.</span><span class="n">sparkContext</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span><span class="p">:(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">),</span> <span class="s1">&#39;C&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svr</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">svr</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="go">GridSearchCV(cv=None, error_score=...,</span>
<span class="go">       estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,</span>
<span class="go">                     decision_function_shape=..., degree=..., gamma=...,</span>
<span class="go">                     kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">                     random_state=None, shrinking=True, tol=...,</span>
<span class="go">                     verbose=False),</span>
<span class="go">       fit_params={}, iid=..., n_jobs=1,</span>
<span class="go">       param_grid=..., pre_dispatch=..., refit=...,</span>
<span class="go">       scoring=..., verbose=...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="gp">... </span>                            
<span class="go">[&#39;mean_fit_time&#39;, &#39;mean_score_time&#39;, &#39;mean_test_score&#39;,...</span>
<span class="go"> &#39;mean_train_score&#39;, &#39;param_C&#39;, &#39;param_kernel&#39;, &#39;params&#39;,...</span>
<span class="go"> &#39;rank_test_score&#39;, &#39;split0_test_score&#39;,...</span>
<span class="go"> &#39;split0_train_score&#39;, &#39;split1_test_score&#39;, &#39;split1_train_score&#39;,...</span>
<span class="go"> &#39;split2_test_score&#39;, &#39;split2_train_score&#39;,...</span>
<span class="go"> &#39;std_fit_time&#39;, &#39;std_score_time&#39;, &#39;std_test_score&#39;, &#39;std_train_score&#39;...]</span>
</pre></div>
</div>
<dl class="docutils">
<dt><a href="#id1"><span class="problematic" id="id2">cv_results_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">dict of numpy (masked) ndarrays</span></dt>
<dd><p class="first">A dict with keys as column headers and values as columns, that can be
imported into a pandas <code class="docutils literal"><span class="pre">DataFrame</span></code>.
For instance the below given table</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="17%" />
<col width="19%" />
<col width="27%" />
<col width="5%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">param_kernel</th>
<th class="head">param_gamma</th>
<th class="head">param_degree</th>
<th class="head">split0_test_score</th>
<th class="head">…</th>
<th class="head"><a href="#id3"><span class="problematic" id="id4">rank_</span></a>….</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>‘poly’</td>
<td>–</td>
<td>2</td>
<td>0.8</td>
<td>…</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>‘poly’</td>
<td>–</td>
<td>3</td>
<td>0.7</td>
<td>…</td>
<td>4</td>
</tr>
<tr class="row-even"><td>‘rbf’</td>
<td>0.1</td>
<td>–</td>
<td>0.8</td>
<td>…</td>
<td>3</td>
</tr>
<tr class="row-odd"><td>‘rbf’</td>
<td>0.2</td>
<td>–</td>
<td>0.9</td>
<td>…</td>
<td>1</td>
</tr>
</tbody>
</table>
</div></blockquote>
<dl class="docutils">
<dt>will be represented by a <code class="docutils literal"><span class="pre">cv_results_</span></code> dict of::</dt>
<dd><p class="first">{
‘param_kernel’: masked_array(data = [‘poly’, ‘poly’, ‘rbf’, ‘rbf’],</p>
<blockquote>
<div>mask = [False False False False]…)</div></blockquote>
<dl class="docutils">
<dt>‘param_gamma’: masked_array(data = [– – 0.1 0.2],</dt>
<dd>mask = [ True  True False False]…),</dd>
<dt>‘param_degree’: masked_array(data = [2.0 3.0 – –],</dt>
<dd>mask = [False False  True  True]…),</dd>
</dl>
<p class="last">‘split0_test_score’  : [0.8, 0.7, 0.8, 0.9],
‘split1_test_score’  : [0.82, 0.5, 0.7, 0.78],
‘mean_test_score’    : [0.81, 0.60, 0.75, 0.82],
‘std_test_score’     : [0.02, 0.01, 0.03, 0.03],
‘rank_test_score’    : [2, 4, 3, 1],
‘split0_train_score’ : [0.8, 0.9, 0.7],
‘split1_train_score’ : [0.82, 0.5, 0.7],
‘mean_train_score’   : [0.81, 0.7, 0.7],
‘std_train_score’    : [0.03, 0.03, 0.04],
‘mean_fit_time’      : [0.73, 0.63, 0.43, 0.49],
‘std_fit_time’       : [0.01, 0.02, 0.01, 0.01],
‘mean_score_time’    : [0.007, 0.06, 0.04, 0.04],
‘std_score_time’     : [0.001, 0.002, 0.003, 0.005],
‘params’             : [{‘kernel’: ‘poly’, ‘degree’: 2}, …],
}</p>
</dd>
</dl>
<p class="last">NOTE that the key <code class="docutils literal"><span class="pre">'params'</span></code> is used to store a list of parameter
settings dict for all the parameter candidates.
The <code class="docutils literal"><span class="pre">mean_fit_time</span></code>, <code class="docutils literal"><span class="pre">std_fit_time</span></code>, <code class="docutils literal"><span class="pre">mean_score_time</span></code> and
<code class="docutils literal"><span class="pre">std_score_time</span></code> are all in seconds.</p>
</dd>
<dt><a href="#id5"><span class="problematic" id="id6">best_estimator_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data. Not available if refit=False.</dd>
<dt><a href="#id7"><span class="problematic" id="id8">best_score_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of best_estimator on the left out data.</dd>
<dt><a href="#id9"><span class="problematic" id="id10">best_params_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameter setting that gave the best results on the hold out data.</dd>
<dt><a href="#id11"><span class="problematic" id="id12">best_index_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The index (of the <code class="docutils literal"><span class="pre">cv_results_</span></code> arrays) which corresponds to the best
candidate parameter setting.
The dict at <code class="docutils literal"><span class="pre">search.cv_results_['params'][search.best_index_]</span></code> gives
the parameter setting for the best model, that gives the highest
mean score (<code class="docutils literal"><span class="pre">search.best_score_</span></code>).</dd>
<dt><a href="#id13"><span class="problematic" id="id14">scorer_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">function</span></dt>
<dd>Scorer function used on the held out data to choose the best
parameters for the model.</dd>
<dt><a href="#id15"><span class="problematic" id="id16">n_splits_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of cross-validation splits (folds/iterations).</dd>
</dl>
<p>The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.
If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
point in the grid (and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<dl class="docutils">
<dt><code class="xref py py-class docutils literal"><span class="pre">ParameterGrid</span></code>:</dt>
<dd>generates all the combinations of a hyperparameter grid.</dd>
<dt><code class="xref py py-func docutils literal"><span class="pre">sklearn.model_selection.train_test_split()</span></code>:</dt>
<dd>utility function to split the data into a development set usable
for fitting a GridSearchCV instance and an evaluation set for
its final evaluation.</dd>
<dt><code class="xref py py-func docutils literal"><span class="pre">sklearn.metrics.make_scorer()</span></code>:</dt>
<dd>Make a scorer from a performance metric or loss function.</dd>
</dl>
<dl class="method">
<dt id="spark_sklearn.GridSearchCV.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y=None</em>, <em>groups=None</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.GridSearchCV.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Run fit with all sets of parameters.</p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples, n_features]</span></dt>
<dd>Training vector, where n_samples is the number of samples and
n_features is the number of features.</dd>
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples] or [n_samples, n_output], optional</span></dt>
<dd>Target relative to X for classification or regression;
None for unsupervised learning.</dd>
<dt>groups <span class="classifier-delimiter">:</span> <span class="classifier">array-like, with shape (n_samples,), optional</span></dt>
<dd>Group labels for the samples used while splitting the dataset into
train/test set.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="spark_sklearn.gapply">
<code class="descclassname">spark_sklearn.</code><code class="descname">gapply</code><span class="sig-paren">(</span><em>grouped_data</em>, <em>func</em>, <em>schema</em>, <em>*cols</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.gapply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the function <code class="docutils literal"><span class="pre">func</span></code> to data grouped by key. In particular, given a dataframe
grouped by some set of key columns key1, key2, …, keyn, this method groups all the values
for each row with the same key columns into a single Pandas dataframe and by default invokes
<code class="docutils literal"><span class="pre">func((key1,</span> <span class="pre">key2,</span> <span class="pre">...,</span> <span class="pre">keyn),</span> <span class="pre">values)</span></code> where the number and order of the key arguments is
determined by columns on which this instance’s parent <code class="xref py py-class docutils literal"><span class="pre">DataFrame</span></code> was grouped and
<code class="docutils literal"><span class="pre">values</span></code> is a <code class="docutils literal"><span class="pre">pandas.DataFrame</span></code> of columns selected by <code class="docutils literal"><span class="pre">cols</span></code>, in that order.</p>
<p>If there is only one key then the key tuple is automatically unpacked, with
<code class="docutils literal"><span class="pre">func(key,</span> <span class="pre">values)</span></code> called.</p>
<p><code class="docutils literal"><span class="pre">func</span></code> is expected to return a <code class="docutils literal"><span class="pre">pandas.DataFrame</span></code> of the specified schema <code class="docutils literal"><span class="pre">schema</span></code>,
which should be of type <code class="xref py py-class docutils literal"><span class="pre">StructType</span></code> (output columns are of this name and order).</p>
<p>If <code class="docutils literal"><span class="pre">spark.conf.get(&quot;spark.sql.retainGroupColumns&quot;)</span></code> is not <code class="docutils literal"><span class="pre">u'true'</span></code>, then <code class="docutils literal"><span class="pre">func</span></code> is
called with an empty key tuple (note it is set to <code class="docutils literal"><span class="pre">u'true'</span></code> by default).</p>
<p>If no <code class="docutils literal"><span class="pre">cols</span></code> are specified, then all grouped columns will be offered, in the order of the
columns in the original dataframe. In either case, the Pandas columns will be named
according to the DataFrame column names.</p>
<p>The order of the rows passed in as Pandas rows is not guaranteed to be stable relative to
the original row order.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Note:</th><td class="field-body"><p class="first">Users must ensure that the grouped values for every group must fit entirely in memory.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Note:</th><td class="field-body"><p class="first">This method is only available if Pandas is installed.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>func</strong> – a two argument function, which may be either a lambda or named function</li>
<li><strong>schema</strong> – the return schema for <code class="docutils literal"><span class="pre">func</span></code>, a <code class="xref py py-class docutils literal"><span class="pre">StructType</span></code></li>
<li><strong>cols</strong> – list of column names (string only)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first simple">
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">&quot;*&quot;</span></code> is in <code class="docutils literal"><span class="pre">cols</span></code></li>
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">cols</span></code> contains duplicates</li>
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">schema</span></code> is not a <code class="xref py py-class docutils literal"><span class="pre">StructType</span></code></li>
<li><strong>ImportError</strong> – if <code class="docutils literal"><span class="pre">pandas</span></code> module is not installed</li>
<li><strong>ImportError</strong> – if <code class="docutils literal"><span class="pre">pandas</span></code> version is too old (less than 0.7.1)</li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the new <code class="xref py py-class docutils literal"><span class="pre">DataFrame</span></code> with the original key columns replicated for each returned
value in each group’s resulting pandas dataframe, the schema being the original key
schema prepended to <code class="docutils literal"><span class="pre">schema</span></code>, where all the resulting groups’ rows are concatenated.
Of course, if retaining group columns is disabled, then the output will exactly match
<code class="docutils literal"><span class="pre">schema</span></code> since no keys can be prepended.</p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">SparkSession</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.group_apply</span> <span class="k">import</span> <span class="n">gapply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.util</span> <span class="k">import</span> <span class="n">createLocalSparkSession</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span> <span class="o">=</span> <span class="n">createLocalSparkSession</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">course</span><span class="o">=</span><span class="s2">&quot;dotNET&quot;</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="mi">2012</span><span class="p">,</span> <span class="n">earnings</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span>
<span class="gp">... </span>                      <span class="n">Row</span><span class="p">(</span><span class="n">course</span><span class="o">=</span><span class="s2">&quot;Java&quot;</span><span class="p">,</span>   <span class="n">year</span><span class="o">=</span><span class="mi">2012</span><span class="p">,</span> <span class="n">earnings</span><span class="o">=</span><span class="mi">20000</span><span class="p">),</span>
<span class="gp">... </span>                      <span class="n">Row</span><span class="p">(</span><span class="n">course</span><span class="o">=</span><span class="s2">&quot;dotNET&quot;</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="mi">2012</span><span class="p">,</span> <span class="n">earnings</span><span class="o">=</span><span class="mi">5000</span><span class="p">),</span>
<span class="gp">... </span>                      <span class="n">Row</span><span class="p">(</span><span class="n">course</span><span class="o">=</span><span class="s2">&quot;dotNET&quot;</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="mi">2013</span><span class="p">,</span> <span class="n">earnings</span><span class="o">=</span><span class="mi">48000</span><span class="p">),</span>
<span class="gp">... </span>                      <span class="n">Row</span><span class="p">(</span><span class="n">course</span><span class="o">=</span><span class="s2">&quot;Java&quot;</span><span class="p">,</span>   <span class="n">year</span><span class="o">=</span><span class="mi">2013</span><span class="p">,</span> <span class="n">earnings</span><span class="o">=</span><span class="mi">30000</span><span class="p">)])</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;course&quot;</span><span class="p">,</span> <span class="s2">&quot;year&quot;</span><span class="p">,</span> <span class="s2">&quot;earnings&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">yearlyMedian</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">vals</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">all_years</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">vals</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="c1"># Note that interpolation is performed, so we need to cast back to int.</span>
<span class="gp">... </span>    <span class="n">yearly_median</span> <span class="o">=</span> <span class="p">[(</span><span class="n">year</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">vals</span><span class="p">[</span><span class="s1">&#39;earnings&#39;</span><span class="p">][</span><span class="n">vals</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">year</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()))</span>
<span class="gp">... </span>                     <span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">all_years</span><span class="p">]</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">yearly_median</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">newSchema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;year&quot;</span><span class="p">,</span> <span class="n">LongType</span><span class="p">())</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;median_earnings&quot;</span><span class="p">,</span> <span class="n">LongType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gapply</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;course&quot;</span><span class="p">),</span> <span class="n">yearlyMedian</span><span class="p">,</span> <span class="n">newSchema</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;median_earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+------+----+---------------+</span>
<span class="go">|course|year|median_earnings|</span>
<span class="go">+------+----+---------------+</span>
<span class="go">|dotNET|2012|           7500|</span>
<span class="go">|  Java|2012|          20000|</span>
<span class="go">|  Java|2013|          30000|</span>
<span class="go">|dotNET|2013|          48000|</span>
<span class="go">+------+----+---------------+</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">twoKeyYearlyMedian</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">vals</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">([(</span><span class="nb">int</span><span class="p">(</span><span class="n">vals</span><span class="p">[</span><span class="s2">&quot;earnings&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()),)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">newSchema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="s2">&quot;earnings&quot;</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gapply</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;course&quot;</span><span class="p">,</span> <span class="s2">&quot;year&quot;</span><span class="p">),</span> <span class="n">twoKeyYearlyMedian</span><span class="p">,</span> <span class="n">newSchema</span><span class="p">,</span> <span class="s2">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+------+----+--------+</span>
<span class="go">|course|year|earnings|</span>
<span class="go">+------+----+--------+</span>
<span class="go">|dotNET|2012|    7500|</span>
<span class="go">|  Java|2012|   20000|</span>
<span class="go">|  Java|2013|   30000|</span>
<span class="go">|dotNET|2013|   48000|</span>
<span class="go">+------+----+--------+</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">();</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">_instantiatedContext</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</dd></dl>

<span class="target" id="module-spark_sklearn.keyed_models"></span><div class="section" id="keyed-models">
<h2>Keyed Models<a class="headerlink" href="#keyed-models" title="Permalink to this headline">¶</a></h2>
<p>The use case that this addresses is where a client has a dataset with many keys - the distribution
of which is such that the total number of rows for with a shared key value can be
contained completely in memory on a single machine.</p>
<p>This assumption is particularly enabling because clients may wish to apply more intricate
single-machine models (such as a scikit-learn estimator) to every user.</p>
<p>The API provided here generalizes the scikit-learn estimator interface to the Spark ML one; in
particular, it allows clients to train their scikit-learn estimators in parallel over a grouped
and aggregated dataframe.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.ml.linalg</span> <span class="k">import</span> <span class="n">Vectors</span><span class="p">,</span> <span class="n">Matrices</span><span class="p">,</span> <span class="n">MatrixUDT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">udf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">SparkSession</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.util</span> <span class="k">import</span> <span class="n">createLocalSparkSession</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spark_sklearn.keyed_models</span> <span class="k">import</span> <span class="n">KeyedEstimator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span> <span class="o">=</span> <span class="n">createLocalSparkSession</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="n">user</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">**</span> <span class="mi">3</span><span class="p">]),</span>
<span class="gp">... </span>                             <span class="mf">0.0</span> <span class="o">+</span> <span class="n">user</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">i</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">... </span>                            <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;features&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s2">&quot;5 &lt; y and y &lt; 10&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-------------+---+</span>
<span class="go">|key|     features|  y|</span>
<span class="go">+---+-------------+---+</span>
<span class="go">|  0|[1.0,1.0,1.0]|6.0|</span>
<span class="go">|  1|[1.0,1.0,1.0]|7.0|</span>
<span class="go">|  2|[1.0,1.0,1.0]|8.0|</span>
<span class="go">+---+-------------+---+</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">km</span> <span class="o">=</span> <span class="n">KeyedEstimator</span><span class="p">(</span><span class="n">sklearnEstimator</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">yCol</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">printFloat</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">printModel</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">coef</span> <span class="o">=</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">printFloat</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span>
<span class="gp">... </span>    <span class="n">intercept</span> <span class="o">=</span> <span class="n">printFloat</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="s2">&quot;intercept: </span><span class="si">{}</span><span class="s2"> coef: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">intercept</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">km</span><span class="o">.</span><span class="n">keyedModels</span><span class="o">.</span><span class="n">columns</span>
<span class="go">[&#39;key&#39;, &#39;estimator&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">printedModels</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">printModel</span><span class="p">)(</span><span class="s2">&quot;estimator&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;linear fit&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">km</span><span class="o">.</span><span class="n">keyedModels</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">printedModels</span><span class="p">)</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">+---+----------------------------------------+</span>
<span class="go">|key|linear fit                              |</span>
<span class="go">+---+----------------------------------------+</span>
<span class="go">|0  |intercept: 0.00 coef: [1.00, 2.00, 3.00]|</span>
<span class="go">|1  |intercept: 1.00 coef: [1.00, 2.00, 3.00]|</span>
<span class="go">|2  |intercept: 2.00 coef: [1.00, 2.00, 3.00]|</span>
<span class="go">+---+----------------------------------------+</span>
</pre></div>
</div>
<p>Now that we have generated a linear model for each key, we can apply it to keyed test data.
In the following, we only show one point for simplicity, but the test data can contain multiple
points for multiple different keys.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))])</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">km</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">udf</span><span class="p">(</span><span class="n">printFloat</span><span class="p">)(</span><span class="s2">&quot;output&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+--------------+------+</span>
<span class="go">|key|      features|output|</span>
<span class="go">+---+--------------+------+</span>
<span class="go">|  0|[3.0,1.0,-1.0]|  2.00|</span>
<span class="go">+---+--------------+------+</span>
</pre></div>
</div>
<p>Suppose we wanted to perform key-based clustering. The most common use case would require just
fitting our model.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">km</span> <span class="o">=</span> <span class="n">KeyedEstimator</span><span class="p">(</span><span class="n">sklearnEstimator</span><span class="o">=</span><span class="n">KMeans</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">getCentroids</span><span class="p">(</span><span class="n">kmeans</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">Matrices</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">centroids</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">getCentroids</span><span class="p">,</span> <span class="n">MatrixUDT</span><span class="p">())(</span><span class="s2">&quot;estimator&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;centroids&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">km</span><span class="o">.</span><span class="n">keyedModels</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+--------------------+</span>
<span class="go">|key|           centroids|</span>
<span class="go">+---+--------------------+</span>
<span class="go">|  0|4.0   64.0  3.5  ...|</span>
<span class="go">|  1|4.0   64.0  3.5  ...|</span>
<span class="go">|  2|4.0   64.0  3.5  ...|</span>
<span class="go">+---+--------------------+</span>
</pre></div>
</div>
<p>Usually, this is all we want. In the case of <code class="docutils literal"><span class="pre">KMeans</span></code>, we can also predict cluster labels,
since the scikit-learn estimator provides this functionality. Note this is not the case for
some other clusterers, such as <code class="docutils literal"><span class="pre">DBSCAN</span></code>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">km</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;cluster label&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+--------------+-------------+</span>
<span class="go">|key|      features|cluster label|</span>
<span class="go">+---+--------------+-------------+</span>
<span class="go">|  0|[3.0,1.0,-1.0]|            1|</span>
<span class="go">+---+--------------+-------------+</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">();</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">_instantiatedContext</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># clear hidden SparkContext for reuse</span>
</pre></div>
</div>
<dl class="class">
<dt id="spark_sklearn.keyed_models.KeyedEstimator">
<em class="property">class </em><code class="descclassname">spark_sklearn.keyed_models.</code><code class="descname">KeyedEstimator</code><span class="sig-paren">(</span><em>sklearnEstimator=None, keyCols=['key'], xCol='features', outputCol='output', yCol=None, estimatorType=None</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">pyspark.ml.base.Estimator</span></code></p>
<p>A <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedEstimator" title="spark_sklearn.keyed_models.KeyedEstimator"><code class="xref py py-class docutils literal"><span class="pre">KeyedEstimator</span></code></a> provides an interface for training per-key scikit-learn estimators.</p>
<p>The <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedEstimator" title="spark_sklearn.keyed_models.KeyedEstimator"><code class="xref py py-class docutils literal"><span class="pre">KeyedEstimator</span></code></a> can be part of any Spark ML pipeline provided the columns are
appropriately matched.</p>
<p>Currently, the class provides a generalization for scikit-learn transformers, clusterers,
and predictors. Because these scikit-learn estimators
all derive from the same base type (yielding the same API), yet have different expectations
for what methods should be called and with what arguments, this class enumerates three different
types of behavior:</p>
<ol class="arabic">
<li><p class="first"><code class="docutils literal"><span class="pre">&quot;transformer&quot;</span></code></p>
<blockquote>
<div><p>Examples: <code class="docutils literal"><span class="pre">sklearn.decomposition.PCA</span></code>, <code class="docutils literal"><span class="pre">sklearn.cluster.KMeans</span></code></p>
<p>In this case, the estimator will aggregate the all input features for a given key into a
<cite>NxD</cite> data matrix, where <cite>N</cite> is the number of rows with the given key and <cite>D</cite> is the
feature space dimensionality; let this matrix be <code class="docutils literal"><span class="pre">X</span></code>.</p>
<p>For each such key and data matrix pair, a clone of the parameter estimator is fitted with
<code class="docutils literal"><span class="pre">estimator.fit(X)</span></code>, inducing a mapping between keys and fitted estimators: this produces
a fitted transformer <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedModel" title="spark_sklearn.keyed_models.KeyedModel"><code class="xref py py-class docutils literal"><span class="pre">KeyedModel</span></code></a>, whose Spark ML <code class="docutils literal"><span class="pre">transform()</span></code> method generates an
output column by applying each key’s fitted scikit-learn estimator’s own <code class="docutils literal"><span class="pre">transform</span></code>
method.</p>
<p>The output column type for transformers will always be a <code class="xref py py-class docutils literal"><span class="pre">DenseVector</span></code>.</p>
</div></blockquote>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">&quot;clusterer&quot;</span></code></p>
<blockquote>
<div><p>Examples: <code class="docutils literal"><span class="pre">sklearn.cluster.DBSCAN</span></code>, <code class="docutils literal"><span class="pre">sklearn.cluster.KMeans</span></code></p>
<p>As before, the data will be aggregated into a design matrix <code class="docutils literal"><span class="pre">X</span></code>, and
<code class="docutils literal"><span class="pre">estimator.fit(X)</span></code> will be called for each key group.</p>
<p>The difference between a <code class="docutils literal"><span class="pre">&quot;transformer&quot;</span></code> and <code class="docutils literal"><span class="pre">&quot;clusterer&quot;</span></code> lies in their prediction
behavior: a clusterer will call <code class="docutils literal"><span class="pre">estimator.predict()</span></code> whereas a transformer refers
to the <code class="docutils literal"><span class="pre">transform</span></code> method.</p>
<p>The output column type for clusterers will always be of <code class="xref py py-class docutils literal"><span class="pre">LongType</span></code>.</p>
</div></blockquote>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">&quot;predictor&quot;</span></code></p>
<blockquote>
<div><p>Examples: <code class="docutils literal"><span class="pre">sklearn.svm.LinearSVC</span></code>, <code class="docutils literal"><span class="pre">sklearn.linear_model.ElasticNet</span></code></p>
<p>Here, the estimator will likewise aggregate input features into the data matrix <code class="docutils literal"><span class="pre">X</span></code>.
In addition, the label column will be aggregated in a collated manner, generating
a vector <code class="docutils literal"><span class="pre">y</span></code> for each key. The estimator clone will be fitted with
<code class="docutils literal"><span class="pre">estimator.fit(X,</span> <span class="pre">y)</span></code>.</p>
<p>A predictor <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedModel" title="spark_sklearn.keyed_models.KeyedModel"><code class="xref py py-class docutils literal"><span class="pre">KeyedModel</span></code></a> transforms its input dataframe by generating an output
column with the output of the estimator’s <code class="docutils literal"><span class="pre">predict</span></code> method.</p>
<p>The output column type for predictors will be the same as the label column (which
must be an <code class="xref py py-class docutils literal"><span class="pre">AtomicType</span></code> (else a <code class="xref py py-class docutils literal"><span class="pre">TypeError</span></code> will be thrown at <code class="docutils literal"><span class="pre">fit()</span></code>-time).</p>
</div></blockquote>
</li>
</ol>
<p>The input column should be numeric or a vector (else a <code class="xref py py-class docutils literal"><span class="pre">TypeError</span></code> will be thrown at
<code class="docutils literal"><span class="pre">fit()</span></code>-time). Don’t use “estimator” as a column name.</p>
<ul class="simple">
<li>In certain cases, a scikit-learn estimator may support both <code class="docutils literal"><span class="pre">&quot;transformer&quot;</span></code> and
<code class="docutils literal"><span class="pre">&quot;clusterer&quot;</span></code> interfaces. <code class="docutils literal"><span class="pre">sklearn.cluster.KMeans</span></code>, for instance, supports both
the cluster-labelling operation <code class="docutils literal"><span class="pre">predict()</span></code> and a transformation into cluster-mean-distance
space. Such ambiguity is resolved by prefering clustering. It may be overriden by manually
specifying the <code class="docutils literal"><span class="pre">estimatorType</span></code> to <code class="docutils literal"><span class="pre">transformer</span></code> in the <code class="docutils literal"><span class="pre">KeyedEstimator</span></code> constructor.</li>
<li>Key-based grouping only occurs during training.
During the transformation/prediction phase of computation, the output is unaggregated:
the number of rows inputted as test data will be equal to the number of rows outputted.</li>
<li><code class="docutils literal"><span class="pre">spark.conf.get(&quot;spark.sql.retainGroupColumns&quot;)</span></code> assumed to be <code class="docutils literal"><span class="pre">u&quot;true&quot;</span></code>.
This is the case by default for Spark 1.4+. This is necessary for both the keyed estimator
and the keyed model.</li>
<li>Estimators trained, persisted, and loaded across different scikit-learn versions
are not guaranteed to work.</li>
</ul>
<p>For all instances, the ordered list of <code class="docutils literal"><span class="pre">keyCols</span></code> determine the set of groups which each
<code class="docutils literal"><span class="pre">sklearnEstimator</span></code> is applied to.</p>
<p>For every unique <code class="docutils literal"><span class="pre">keyCols</span></code> value, the remaining columns are aggregated and used to train
the scikit-learn estimator.</p>
<p><code class="docutils literal"><span class="pre">estimatorType</span></code> inference is conducted as follows: if <code class="docutils literal"><span class="pre">yCol</span></code> is specified, then this is
assumed to be of <code class="docutils literal"><span class="pre">&quot;predictor&quot;</span></code> type, else a <code class="docutils literal"><span class="pre">&quot;transformer&quot;</span></code> or a <code class="docutils literal"><span class="pre">&quot;clusterer&quot;</span></code>,
depending on the estimator having the <code class="docutils literal"><span class="pre">transform()</span></code> or <code class="docutils literal"><span class="pre">fit_predict()</span></code> attributes, with
<code class="docutils literal"><span class="pre">&quot;clusterer&quot;</span></code> being chosen in case both attributes are present.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sklearnEstimator</strong> – An instance of a scikit-learn estimator, with parameters configured
as desired for each user.</li>
<li><strong>keyCols</strong> – Key column names list used to group data to which models are applied, where
order implies lexicographical importance.</li>
<li><strong>xCol</strong> – Name of column of input features used for training and
transformation/prediction.</li>
<li><strong>yCol</strong> – Specifies name of label column for regression or classification pipelines.
Required for predictors, must be unspecified or <code class="docutils literal"><span class="pre">None</span></code> for transformers.</li>
<li><strong>estimatorType</strong> – Identifies the type of scikit-learn estimator being used, which
changes the interface the <code class="docutils literal"><span class="pre">sklearnEstimator</span></code> is expected to have.
This parameter’s value is inferred using reflection by default,
but may be manually overriden.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">sklearnEstimator</span></code> is <code class="docutils literal"><span class="pre">None</span></code>.</li>
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">sklearnEstimator</span></code> does not derive from
<code class="docutils literal"><span class="pre">sklearn.base.BaseEstimator</span></code>.</li>
<li><strong>ValueError</strong> – if <code class="docutils literal"><span class="pre">keyCols</span></code> is empty.</li>
<li><strong>ValueError</strong> – if any column has the name <code class="docutils literal"><span class="pre">&quot;estimator&quot;</span></code></li>
<li><strong>AttributeError</strong> – if reflection checks indicate that parameter estimator is not equipped
with a <code class="docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="spark_sklearn.keyed_models.KeyedEstimator.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>params=None</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedEstimator.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a model to the input dataset with optional parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> – input dataset, which is an instance of <code class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrame</span></code></li>
<li><strong>params</strong> – an optional param map that overrides embedded params. If a list/tuple of
param maps is given, this calls fit on each param map and returns a list of
models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">fitted model(s)</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="spark_sklearn.keyed_models.KeyedEstimator.sklearnEstimatorType">
<code class="descname">sklearnEstimatorType</code><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedEstimator.sklearnEstimatorType" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the estimator type of this keyed estimator</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="spark_sklearn.keyed_models.KeyedModel">
<em class="property">class </em><code class="descclassname">spark_sklearn.keyed_models.</code><code class="descname">KeyedModel</code><span class="sig-paren">(</span><em>sklearnEstimator=None</em>, <em>keyCols=None</em>, <em>xCol=None</em>, <em>outputCol=None</em>, <em>yCol=None</em>, <em>estimatorType=None</em>, <em>keyedSklearnEstimators=None</em>, <em>outputType=None</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">pyspark.ml.base.Model</span></code></p>
<p>Represents a Spark ML Model, generated by a fitted <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedEstimator" title="spark_sklearn.keyed_models.KeyedEstimator"><code class="xref py py-class docutils literal"><span class="pre">KeyedEstimator</span></code></a>.</p>
<p>Wraps fitted scikit-learn estimators - at transformation time transforms the
input for each key using a key-specific model. See <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedEstimator" title="spark_sklearn.keyed_models.KeyedEstimator"><code class="xref py py-class docutils literal"><span class="pre">KeyedEstimator</span></code></a> documentation for
details.</p>
<p>If no estimator is present for a given key at transformation time, the prediction is null.</p>
<p>The constructor is used by <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedEstimator" title="spark_sklearn.keyed_models.KeyedEstimator"><code class="xref py py-class docutils literal"><span class="pre">KeyedEstimator</span></code></a> to generate a <a class="reference internal" href="#spark_sklearn.keyed_models.KeyedModel" title="spark_sklearn.keyed_models.KeyedModel"><code class="xref py py-class docutils literal"><span class="pre">KeyedModel</span></code></a>; it
is not intended for external use.</p>
<dl class="attribute">
<dt id="spark_sklearn.keyed_models.KeyedModel.keyedModels">
<code class="descname">keyedModels</code><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedModel.keyedModels" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Returns the <code class="docutils literal"><span class="pre">keyedSklearnEstimators</span></code> param, a <code class="xref py py-class docutils literal"><span class="pre">DataFrame</span></code> with columns
<code class="docutils literal"><span class="pre">keyCols</span></code> (where each key is unique) and the column <code class="docutils literal"><span class="pre">&quot;estimator&quot;</span></code> containing
the fitted scikit-learn estimator as a <a class="reference internal" href="#spark_sklearn.keyed_models.SparkSklearnEstimator" title="spark_sklearn.keyed_models.SparkSklearnEstimator"><code class="xref py py-class docutils literal"><span class="pre">SparkSklearnEstimator</span></code></a>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="spark_sklearn.keyed_models.KeyedModel.sklearnEstimatorType">
<code class="descname">sklearnEstimatorType</code><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedModel.sklearnEstimatorType" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the estimator type of this keyed model</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="spark_sklearn.keyed_models.KeyedModel.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>dataset</em>, <em>params=None</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.keyed_models.KeyedModel.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms the input dataset with optional parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> – input dataset, which is an instance of <code class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrame</span></code></li>
<li><strong>params</strong> – an optional param map that overrides embedded params.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">transformed dataset</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="spark_sklearn.keyed_models.SparkSklearnEstimator">
<em class="property">class </em><code class="descclassname">spark_sklearn.keyed_models.</code><code class="descname">SparkSklearnEstimator</code><span class="sig-paren">(</span><em>estimator</em><span class="sig-paren">)</span><a class="headerlink" href="#spark_sklearn.keyed_models.SparkSklearnEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p><a class="reference internal" href="#spark_sklearn.keyed_models.SparkSklearnEstimator" title="spark_sklearn.keyed_models.SparkSklearnEstimator"><code class="xref py py-class docutils literal"><span class="pre">SparkSklearnEstimator</span></code></a> is a wrapper for containing scikit-learn estimators in
dataframes - any estimators need to be stored inside the wrapper class to be properly
serialized/deserialized in dataframe operations.</p>
<p>Note any method called on the estimator this object wraps may be called on the wrapper instead.</p>
<p>Initializes with the parameter estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Param:</th><td class="field-body">estimator: scikit-learn estimator to contain.</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="spark_sklearn.keyed_models.SparkSklearnEstimator.estimator">
<code class="descname">estimator</code><a class="headerlink" href="#spark_sklearn.keyed_models.SparkSklearnEstimator.estimator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the underlying estimator</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Welcome to spark_sklearn’s documentation!</a><ul>
<li><a class="reference internal" href="#keyed-models">Keyed Models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">spark_sklearn 0.2.3 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2015, Joseph Bradley, Tim Hunter, Vladimir Feinberg.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.3.
    </div>
  </body>
</html>